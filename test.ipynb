{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "import util.misc as utils\n",
    "import datasets.samplers as samplers\n",
    "from datasets import build_dataset, get_coco_api_from_dataset\n",
    "from engine import evaluate, train_one_epoch\n",
    "from models import build_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import numpy as np\n",
    "from util import box_ops\n",
    "from util.misc import (NestedTensor, nested_tensor_from_tensor_list,\n",
    "                       accuracy, get_world_size, interpolate,\n",
    "                       is_dist_avail_and_initialized, inverse_sigmoid)\n",
    "\n",
    "from models.backbone import build_backbone\n",
    "from models.matcher import build_matcher\n",
    "from models.segmentation import (DETRsegm, PostProcessPanoptic, PostProcessSegm,\n",
    "                           dice_loss, sigmoid_focal_loss)\n",
    "from models.deformable_transformer import build_deforamble_transformer\n",
    "import copy\n",
    "from torchvision.ops.boxes import batched_nms \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###출력헤드 만들기.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim,num_layers):\n",
    "        #중간은 hidden_dim으로 때우나보다.\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers \n",
    "        #self.input_dim = input_dim 다른 메소드에서 사용하면 이렇게 선언\n",
    "        h = [hidden_dim]*(num_layers-1) #마지막 레이어는 출력이라.\n",
    "        self.layers = nn.ModuleList(nn.Linear(n,k) for n,k in zip ([input_dim]+h,h+[output_dim]))\n",
    "    def forward(self,x):\n",
    "        for layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers-1 else layer(x)\n",
    "            #마지막 레이어는 relu안하기.\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IpsHead(nn.Module):\n",
    "    def __init__(self,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(0,1)#batch_size * num_objects x hidden_dim\n",
    "        self.linear2 = nn.Linear(hidden_dim,1)\n",
    "        self.activate = nn.Sigmoid() # 확률로 출력해야하니.\n",
    "        # nn.init.constant_(self.linear2.weight,0)\n",
    "        # nn.init.constant_(self.linear2.bias,0)\n",
    "    def fressze_obj_head(self):\n",
    "        self.obj_head.eval()\n",
    "    def forward(self,x):\n",
    "        out = self.flatten(x)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activate(out)\n",
    "        out = out.unflatten(0,x.shape[:2])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class Deformable(nn.Module):\n",
    "    def __init__(self,backbone,transformer,num_classes,num_queries,num_feature_levels,aux_loss=True,with_box_refine=True,two_stage=True):\n",
    "        super().__init__()\n",
    "        self.num_feature_levels = num_feature_levels\n",
    "        self.backbone = backbone\n",
    "        self.num_queries = num_queries\n",
    "        self.transformer = transformer\n",
    "        hidden_dim = transformer.d_model #들어가는 차원수, 어디에 쓰냐.\n",
    "        self.class_embed = nn.Linear(hidden_dim,num_classes)\n",
    "        self.bbox_embed  = MLP(hidden_dim,hidden_dim,4,3)\n",
    "        self.object_head = IpsHead(hidden_dim)\n",
    "        if not two_stage:\n",
    "            self.query_embed = nn.Embedding(num_queries,hidden_dim*2)\n",
    "        if num_feature_levels > 1:\n",
    "            num_backbone_outs = len(backbone.strides) #backbone의 출력리스트 개수\n",
    "            input_proj_list = []\n",
    "            for _ in range(num_backbone_outs):\n",
    "                in_channels = backbone.num_channels[_]\n",
    "                #각 in_channels받아오기.\n",
    "                input_proj_list.append(\n",
    "                    nn.Sequential(nn.Conv2d(in_channels,hidden_dim,kernel_size=1),nn.GroupNorm(32,hidden_dim),)\n",
    "                )\n",
    "            for _ in range(num_feature_levels - num_backbone_outs):\n",
    "                input_proj_list.append(nn.Sequential(nn.Conv2d(in_channels,hidden_dim,kernel_size=3,stride=2,padding=1),nn.GroupNorm(32,hidden_dim),))\n",
    "                in_channels = hidden_dim\n",
    "            self.input_proj = nn.ModuleList(input_proj_list)\n",
    "        else:\n",
    "            self.input_proj = nn.ModuleList([nn.Sequential(nn.Conv2d(backbone.num_channels[0],hidden_dim,kernel_size=1),nn.GroupNorm(32,hidden_dim))]) #self.num_channels은 중간출력값이 없어 2048일것\n",
    "        #여기까지 backbone setting. 선언 및 초기화.\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        self.aux_loss = aux_loss\n",
    "        self.with_box_refine = with_box_refine\n",
    "        self.two_stage= two_stage\n",
    "        prior_prob =0.01\n",
    "        bias_value = -math.log((1-prior_prob)/prior_prob)\n",
    "        self.class_embed.bias.data = torch.ones(num_classes)*bias_value\n",
    "        nn.init.constant_(self.bbox_embed.layers[-1].weight.data,0)\n",
    "        nn.init.constant_(self.bbox_embed.layers[-1].bias.data,0)\n",
    "        #레이어 편향초기화.\n",
    "        for proj in self.input_proj:\n",
    "            nn.init.xavier_uniform(proj[0].weight,gain=1)\n",
    "            nn.init.constant_(proj[0].bias,0) #proj[0] = Conv.\n",
    "        #region_proposal generation   \n",
    "        num_pred = (transformer.decoder.num_layers+1) if two_stage else transformer.decoder.num_layers\n",
    "        #num_pred는 디코더의 레이어 수.\n",
    "        if with_box_refine:\n",
    "            \n",
    "            self.class_embed = _get_clones(self.class_embed,num_pred)\n",
    "            #디코더의 레이어마다 예측을 해준다.\n",
    "            self.bbox_embed = _get_clones(self.bbox_embed,num_pred)\n",
    "            self.object_head = _get_clones(self.object_head,num_pred)\n",
    "            nn.init.constant_(self.bbox_embed[0].layer[-1].bias.data[2:],-2.0) #초기바운딩 박스예측에서 작은 값을 설정.\n",
    "            self.transformer.decoder.bbox_embed = self.bbox_embed\n",
    "            \n",
    "        else:\n",
    "            nn.init.constant_(self.bbox_embed.layer[-1].bias.data[2:],-2.0)\n",
    "            self.class_embed = nn.ModuleList([self.class_embed for _ in range (num_pred)])\n",
    "            self.bbox_embed = nn.ModuleList([self.bbox_embed for _ in range (num_pred)])\n",
    "            self.transformer.decoder.bbox_embed = None\n",
    "        if two_stage:\n",
    "            #애초에 안넣으면,self.transformer.decoder.class_embed는 None\n",
    "            \n",
    "            self.transformer.decoder.class_embed = self.class_embed\n",
    "            for box_embed in self.bbox_embed:\n",
    "                nn.init.constant_(box_embed.layers[-1].bias.data[2:0],0.0)\n",
    "    def forward(self,samples: NestedTensor):\n",
    "        \"\"\"The forward expects a NestedTensor, which consists of:\n",
    "               - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
    "               - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels\"\"\"\n",
    "               \n",
    "        if not isinstance(samples,NestedTensor):\n",
    "            samples = nested_tensor_from_tensor_list(samples)\n",
    "        features, pos = self.backbone(samples)\n",
    "        \n",
    "        srcs = []\n",
    "        masks = []\n",
    "        for l,feat in enumerate(features):\n",
    "            src,mask = feat.decompose()\n",
    "            srcs.append(self.input_proj[l](src))\n",
    "            masks.append(mask)\n",
    "            assert mask is not None\n",
    "        if self.num_feature_levels > len(srcs):\n",
    "            _len_srcs = len(srcs)\n",
    "            for l in range(_len_srcs,self.num_feature_levels):\n",
    "                if l == _len_srcs:\n",
    "                    src = self.input_proj[l](features[-1].tensors)\n",
    "                else:\n",
    "                    src = self.input_proj[l](srcs[-1])\n",
    "                m = samples.mask\n",
    "                mask = F.interpolate(m[None].float(),size= src.shape[-2:]).to(torch.bool)[0]\n",
    "                pos_l = self.backbone[l](NestedTensor(src,mask)).to(src.dtype)\n",
    "                srcs.append(src)\n",
    "                masks.append(mask)\n",
    "                pos.append(pos_l)\n",
    "        query_embeds = None\n",
    "        if not self.two_stage:\n",
    "            query_embeds = self.query_embed.weight\n",
    "        hs,init_reference,inter_references,enc_outputs_class,enc_outputs_coord_unact = self.transformer(srcs,mask,pos,query_embeds)\n",
    "        #what is hs, reference\n",
    "        outputs_classes = []\n",
    "        outputs_coords = []\n",
    "        \n",
    "        for lvl in range(hs.shape[0]):\n",
    "            if lvl == 0:\n",
    "                reference = init_reference\n",
    "            else:\n",
    "                reference = inter_references[lvl-1]\n",
    "            reference = inverse_sigmoid(reference) #why?\n",
    "            outputs_classes = self.class_embed[lvl](hs[lvl])\n",
    "            tmp = self.bbox_embed[lvl](hs[lvl])\n",
    "            if reference.shape[-1] ==4:\n",
    "                tmp += reference\n",
    "            else:\n",
    "                assert reference.shape[-1] == 2\n",
    "                tmp[...,:2] += tmp.sigmoid()\n",
    "            outputs_coords = tmp.sigmoid()\n",
    "            outputs_coords.append(outputs_coords)\n",
    "            outputs_classes.append(outputs_classes)\n",
    "        outputs_class = torch.stack(outputs_classes)\n",
    "        outputs_coord = torch.stack(outputs_coords)\n",
    "        \n",
    "        out = {'pred_logits':outputs_class[-1],'pred_boxes':outputs_coord[-1]}\n",
    "        if self.aux_loss:\n",
    "            out['aux_outputs'] = self._set_aux_loss(outputs_class,outputs_coord)\n",
    "        if self.two_stage:\n",
    "            enc_outputs_coord = enc_outputs_coord_unact.sigmoid()\n",
    "            #0~1로 정규화됨. \n",
    "            out['enc_outputs'] = {'pred_logits':enc_outputs_class,'pred:boxes':enc_outputs_coord}\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetCriterion(nn.Module):\n",
    "    \"\"\" This class computes the loss for DETR.\n",
    "    The process happens in two steps:\n",
    "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
    "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
    "    \"\"\"\n",
    "    def __init__(self,num_classes,matcher,weight_dict,losses,focal_alpha=0.25):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.losses = losses\n",
    "        self.focal_alpha = focal_alpha\n",
    "        \n",
    "    def loss_labels(self,outputs,targets,indices,num_boxes,log=True):\n",
    "        assert 'pred_logits' in outputs\n",
    "        src_logits = outputs['pred_logits']\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        #idx = batch_idx, src_idx\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J] for t,(_,J) in zip(targets,indices)])\n",
    "        #(t, (src_idx, tgt_idx)) 이거 확인해야돼!\n",
    "        target_classes = torch.full(src_logits.shape[:2],self.num_classes,\n",
    "                                    dtype=torch.int64,\n",
    "                                    device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "        target_classes_onehot = torch.zeros([src_logits.shape[0],src_logits.shape[1],src_logits.shape[2]+1])\n",
    "        target_classes_onehot.scatter_(2,target_classes.unsqueeze(-1),1)\n",
    "        #target_classes 텐서의 마지막 차원에 새로운 차원을 추가.\n",
    "        target_classes_onehot = target_classes_onehot[:,:,:-1]\n",
    "        loss_ce = sigmoid_focal_loss(src_logits,target_classes_onehot,num_boxes,alpha=self.focal_alpha,gamma=2)*src_logits.shape[1]\n",
    "        losses = {'loss_ce':loss_ce}\n",
    "        \n",
    "        if log:\n",
    "            losses['class_error'] = 100 - accuracy(src_logits[idx],target_classes_o)[0]\n",
    "        return losses\n",
    "    def loss_cardinality(self, outputs, targets, indices, num_boxes):#+second_indices\n",
    "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
    "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
    "        \"\"\"\n",
    "        pred_logits = outputs['pred_logits']\n",
    "        device = pred_logits.device\n",
    "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
    "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
    "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
    "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
    "        losses = {'cardinality_error': card_err}\n",
    "        return losses\n",
    "\n",
    "    def loss_boxes(self,outputs,targets,indices,num_boxes):\n",
    "        assert 'pred_boxes' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        #idx가 특정 인덱스인지 단체로있는건지 모르겠음.\n",
    "        src_boxes = outputs['pred_boxes'][idx]\n",
    "        target_boxes = torch.cat([t['boxes'][i] for t,(_,i) in zip(targets,indices)],dim=0)\n",
    "        loss_bbox = F.l1_loss(src_boxes,target_boxes,reduction='none')\n",
    "        losses = {}\n",
    "        losses['loss_bbox'] = loss_bbox.sum()/num_boxes\n",
    "        \n",
    "        loss_giou = 1- torch.diag(box_ops.generalized_box_iou(box_ops.box_cxcywh_to_xyxy(src_boxes),box_ops.box_cxcywh_to_xyxy(target_boxes)))\n",
    "        losses['loss_giou'] = loss_giou.sum()/num_boxes\n",
    "        return losses\n",
    "    \n",
    "    def loss_masks(self, outputs, targets, indices, num_boxes):#tmp_indices,epoch\n",
    "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
    "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
    "        \"\"\"\n",
    "        assert \"pred_masks\" in outputs\n",
    "        \n",
    "        src_idx = self._get_src_permutation_idx(indices)\n",
    "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
    "\n",
    "        src_masks = outputs[\"pred_masks\"]\n",
    "\n",
    "        # TODO use valid to mask invalid areas due to padding in loss\n",
    "        target_masks, valid = nested_tensor_from_tensor_list([t[\"masks\"] for t in targets]).decompose()\n",
    "        target_masks = target_masks.to(src_masks)\n",
    "\n",
    "        src_masks = src_masks[src_idx]\n",
    "        # upsample predictions to the target size\n",
    "        src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n",
    "                                mode=\"bilinear\", align_corners=False)\n",
    "        src_masks = src_masks[:, 0].flatten(1)\n",
    "\n",
    "        target_masks = target_masks[tgt_idx].flatten(1)\n",
    "\n",
    "        losses = {\n",
    "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
    "            \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
    "        }\n",
    "        return losses\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        # permute targets following indices\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "    def get_loss(self,loss,outputs,targets,indices,num_boxes,**kwargs):\n",
    "        loss_map = {\n",
    "            'labels' : self.loss_labels,\n",
    "            'cardinality' : self.loss_cardinality,\n",
    "            'boxes' : self.loss_boxes,\n",
    "            'masks' : self.loss_masks\n",
    "        }\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs,targets,indices,num_boxes,**kwargs)\n",
    "    def forward(self,outputs,targets):\n",
    "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs' and k != 'enc_outputs'}\n",
    "        indices = self.matcher(outputs_without_aux,targets)\n",
    "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
    "        num_boxes = torch.as_tensor([num_boxes],dtype=torch.float,device=next(iter(outputs.values())).device)\n",
    "        num_boxes = torch.clamp(num_boxes/get_world_size(),min=1).item()\n",
    "        \n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            kwargs = {}\n",
    "            losses.update(self.get_loss(loss,outputs,targets,indices,num_boxes,**kwargs))  #lossmap 갱신\n",
    "        if 'aux_outputs' in outputs:\n",
    "            for i,aux_outputs in enumerate(outputs['aux_outputs']):\n",
    "                indices = self.matcher(aux_outputs,targets)\n",
    "                if loss =='masks':\n",
    "                    continue\n",
    "                kwargs = {}\n",
    "                if loss =='labels':\n",
    "                    kwargs['log'] = False\n",
    "                l_dict = self.get_loss(loss,aux_outputs,targets,indices,num_boxes,**kwargs)\n",
    "                l_dict = {k + f'_{i}': v for k,v in l_dict.items()}\n",
    "                losses.update(l_dict)\n",
    "        if 'enc_output' in outputs:\n",
    "            enc_outputs = outputs['enc_outputs']\n",
    "            bin_targets = copy.deepcopy(targets)\n",
    "            for bt in bin_targets:\n",
    "                bt['labels'] = torch.zeros_like(bt['labels']) #no_object로 설정.\n",
    "            indices = self.matcher(enc_outputs,bin_targets)\n",
    "            for loss in self.losses:\n",
    "                if loss == 'masks':\n",
    "                \n",
    "                    continue\n",
    "                kwargs = {}\n",
    "                if loss == 'labels':\n",
    "                    kwargs['log'] = False\n",
    "                l_dict = self.get_loss(loss, enc_outputs, bin_targets, indices, num_boxes, **kwargs)\n",
    "                #이때 target은 0으로 차있는 텐서/\n",
    "                l_dict = {k + f'_enc': v for k, v in l_dict.items()}\n",
    "                losses.update(l_dict)\n",
    "                \n",
    "            return losses\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0155,  0.7537,  0.1728,  1.0055],\n",
      "        [ 0.6616, -0.3418,  1.4101,  1.0494],\n",
      "        [-0.2863, -0.4926, -0.6691,  0.4535],\n",
      "        ...,\n",
      "        [-0.6731, -0.6795, -0.7950, -0.0796],\n",
      "        [ 0.1980,  0.5483, -0.5475,  0.4446],\n",
      "        [-0.7517, -0.4201, -0.2257, -0.9833]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[[0.6128],\n",
      "         [0.2261],\n",
      "         [0.4610],\n",
      "         [0.5335],\n",
      "         [0.5371],\n",
      "         [0.4147],\n",
      "         [0.5092],\n",
      "         [0.5341],\n",
      "         [0.3849],\n",
      "         [0.5527],\n",
      "         [0.4893],\n",
      "         [0.6785],\n",
      "         [0.2209],\n",
      "         [0.4640],\n",
      "         [0.6745],\n",
      "         [0.2606],\n",
      "         [0.4794],\n",
      "         [0.6315],\n",
      "         [0.2708],\n",
      "         [0.5134]],\n",
      "\n",
      "        [[0.6860],\n",
      "         [0.7088],\n",
      "         [0.6086],\n",
      "         [0.4202],\n",
      "         [0.3929],\n",
      "         [0.5026],\n",
      "         [0.6284],\n",
      "         [0.4770],\n",
      "         [0.4755],\n",
      "         [0.6593],\n",
      "         [0.4052],\n",
      "         [0.3331],\n",
      "         [0.3536],\n",
      "         [0.7010],\n",
      "         [0.7606],\n",
      "         [0.4901],\n",
      "         [0.5143],\n",
      "         [0.5586],\n",
      "         [0.5541],\n",
      "         [0.4319]],\n",
      "\n",
      "        [[0.5830],\n",
      "         [0.7376],\n",
      "         [0.4242],\n",
      "         [0.5547],\n",
      "         [0.6535],\n",
      "         [0.5346],\n",
      "         [0.6189],\n",
      "         [0.7398],\n",
      "         [0.2322],\n",
      "         [0.7741],\n",
      "         [0.7717],\n",
      "         [0.4803],\n",
      "         [0.3009],\n",
      "         [0.5873],\n",
      "         [0.2073],\n",
      "         [0.3469],\n",
      "         [0.8535],\n",
      "         [0.3910],\n",
      "         [0.5514],\n",
      "         [0.5585]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class_embed = nn.Linear(256,4)\n",
    "input_tensor = torch.randn(256, 256)\n",
    "output_tensor = class_embed(input_tensor)\n",
    "print(output_tensor)\n",
    "class_object_head = IpsHead(256)\n",
    "input_tensor2 = torch.randn(3,20,256)\n",
    "output_tensor2 = class_object_head(input_tensor2)\n",
    "print(output_tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이거머임'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
